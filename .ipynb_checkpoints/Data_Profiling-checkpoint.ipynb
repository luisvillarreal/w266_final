{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, re, json, time\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "import scipy.sparse\n",
    "import nltk\n",
    "from w266_common import utils, vocabulary, tf_embed_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Tickers and Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two different data folders ('data' and 'data2'). This is because we first got all the tweets that mentioned name of stock as per the convention $ (ticker). \n",
    "\n",
    "Afterwards we got tweets that include names of companies along with their ticker symbols in the tweets. For example, we gathered any tweet that mentioned Amazon, and any tweet that mention $ (AMZN). This substantially increased our tweet count.\n",
    "\n",
    "Our tweets dataset consist of tweets pertaining to S&P500 Top 50 companies only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the files in folder 'data'\n",
    "files = []\n",
    "for file in os.listdir('data'):\n",
    "    if file.endswith('.csv'):\n",
    "        files.append(os.path.join('data', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe from files in 'data' folder\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in files:\n",
    "    with open(filename, 'r') as file:\n",
    "        ticker = filename.split('_')[-1][:-4]\n",
    "        line = file.readline()\n",
    "        line = file.readline()\n",
    "        while(line):\n",
    "            ts =line.split(';\"')[0][1:].split(';')[0]\n",
    "            tweet = line.split(';\"')[1].split('\";')[0]\n",
    "            tweet_id = line.split('/')[-1][:-1] \n",
    "            line=file.readline()\n",
    "            data.append([tweet_id,ticker,ts,tweet])\n",
    "        \n",
    "#df = pd.DataFrame(data, columns=['tweet_id','ticker','timestamp','tweet'])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the files in 'data2' folder\n",
    "files1 = []\n",
    "for file in os.listdir('data2'):\n",
    "    if file.endswith('.csv'):\n",
    "        files1.append(os.path.join('data2', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe for files in 'data2' folder\n",
    "\n",
    "\n",
    "for filename in files1:\n",
    "    with open(filename, 'r') as file:\n",
    "        ticker = filename.split('_')[-1][:-4]\n",
    "        line = file.readline()\n",
    "        line = file.readline()\n",
    "        while(line):\n",
    "            ts =line.split(';\"')[0][1:].split(';')[0]\n",
    "            tweet = line.split(';\"')[1].split('\";')[0]\n",
    "            tweet_id = line.split('/')[-1][:-1] \n",
    "            line=file.readline()\n",
    "            data.append([tweet_id,ticker,ts,tweet])\n",
    "        \n",
    "df = pd.DataFrame(data, columns=['tweet_id','ticker','timestamp','tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from shapes of dataframes below, our first dataframe has considerably less tweets than the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3144677, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1108873286585331713</td>\n",
       "      <td>INTC</td>\n",
       "      <td>2019-03-21 23:29</td>\n",
       "      <td>$ AAPL $ TRV $ MSFT $ HD $ DWDP $ INTC $ NKE $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1108872350223736833</td>\n",
       "      <td>INTC</td>\n",
       "      <td>2019-03-21 23:25</td>\n",
       "      <td>$ INTC Intel - Q3 2019 Intel Corporation Earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1108872312034492417</td>\n",
       "      <td>INTC</td>\n",
       "      <td>2019-03-21 23:25</td>\n",
       "      <td>$ INTC # Intel # Intelligence # ImplementingCl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1108872250177060865</td>\n",
       "      <td>INTC</td>\n",
       "      <td>2019-03-21 23:25</td>\n",
       "      <td>$ INTC Intel - Q2 2019 Intel Corporation Earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1108872157822636038</td>\n",
       "      <td>INTC</td>\n",
       "      <td>2019-03-21 23:24</td>\n",
       "      <td>$ INTC Intel - Q1 2019 Intel Corporation Earni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id ticker         timestamp  \\\n",
       "0  1108873286585331713   INTC  2019-03-21 23:29   \n",
       "1  1108872350223736833   INTC  2019-03-21 23:25   \n",
       "2  1108872312034492417   INTC  2019-03-21 23:25   \n",
       "3  1108872250177060865   INTC  2019-03-21 23:25   \n",
       "4  1108872157822636038   INTC  2019-03-21 23:24   \n",
       "\n",
       "                                               tweet  \n",
       "0  $ AAPL $ TRV $ MSFT $ HD $ DWDP $ INTC $ NKE $...  \n",
       "1  $ INTC Intel - Q3 2019 Intel Corporation Earni...  \n",
       "2  $ INTC # Intel # Intelligence # ImplementingCl...  \n",
       "3  $ INTC Intel - Q2 2019 Intel Corporation Earni...  \n",
       "4  $ INTC Intel - Q1 2019 Intel Corporation Earni...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_dict = {'Union':'UNP', 'Disney':'DIS', 'Nike':'NKE', 'Chevron':'CVX',\n",
    "                  'Intel':'INTC', 'Bank':'BAC', 'Pepsi':'PEP', 'Amgen':'AMGN',\n",
    "                  'AT&T':'T', 'Procter':'PG', 'Microsoft':'MSFT', 'Wells':'WFG',\n",
    "                  'Walmart':'WMT', 'Citigroup':'C', 'Verizon':'VZ', 'Exxon-Mobil':'XOM',\n",
    "                  'Apple':'AAPL', 'Mastercard':'MA', 'Merck':'MRK', 'Boeing':'BA', \n",
    "                  'Comcast':'CMCSA', 'Salesforce':'CRM', 'Home':'HD', 'Berkshire':'BRK',\n",
    "                  'Cisco':'CSCO', 'ATT':'T', 'Dow':'DWDP', 'Coca-Cola':'KO', 'Visa':'V',\n",
    "                  'Facebook':'FB', 'Johnson':'JNJ', 'Abbott':'ABBT', 'Broadcom':'AVGO',\n",
    "                  '3M':'MMM', 'Pfizer':'PFE', 'Amazon':'AMZN', 'Honeywell':'HON', 'Adobe':'ADBE',\n",
    "                  'Google':'GOOG', 'Netflix':'NFLX', 'Eli':'LLY', 'Phillips':'PM', 'United':'UNP',\n",
    "                  'AbbVie':'ABBV', 'McDonald':'MCD', 'JP':'JPM', 'Paypal':'PYPL', 'Oracle':'ORCL', \n",
    "                  'Medtronic':'MDT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate tickers with company names\n",
    "\n",
    "for key, value in companies_dict.items():\n",
    "    df.loc[df['ticker'] == key, 'ticker'] = value\n",
    "    #combined_df.at[index, 'ticker'] = companies_dict[row['ticker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Nasdaq Stock Symbols\n",
    "os.system(\"curl --ftp-ssl anonymous:jupi@jupi.com \"\n",
    "          \"ftp://ftp.nasdaqtrader.com/SymbolDirectory/nasdaqlisted.txt \"\n",
    "          \"> nasdaq.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html data-adblockkey=\"MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBANDrp2lz7AOmADaN8tA50LsWcjLFyQFcb/P2Txc58oYOeILb3vBw7J6f4pamkAQVSQuqYsKx3YzdUHCvbVZvFUsCAwEAAQ==_pxrolOu5yyzUS0zMNgm01mNbFq+3Njr33BJOzQ+JCg65FKwTyH/JswWy8wg/VocffrVzS1j7NXxyt+jJ2gFZsQ==\"><head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"><title></title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><meta name=\"description\" content=\"See related links to what you are looking for.\"/></head><!--[if IE 6 ]><body class=\"ie6\"><![endif]--><!--[if IE 7 ]><body class=\"ie7\"><![endif]--><!--[if IE 8 ]><body class=\"ie8\"><![endif]--><!--[if IE 9 ]><body class=\"ie9\"><![endif]--><!--[if (gt IE 9)|!(IE)]> --><body><!--<![endif]--><script type=\"text/javascript\">g_pb=(function(){var\n",
      "DT=document,azy=location,DD=DT.createElement('script'),aAA=false,LU;DD.defer=true;DD.async=true;DD.src=\"//www.google.com/adsense/domains/caf.js\";DD.onerror=function(){if(azy.search!=='?z'){azy.href='/?z';}};DD.onload=DD.onreadystatechange=function(){if(!aAA&&LU){if(!window['googleNDT_']){}\n",
      "LU(google.ads.domains.Caf);}\n",
      "aAA=true;};DT.body.appendChild(DD);return{azl:function(n$){if(aAA)\n",
      "n$(google.ads.domains.Caf);else\n",
      "LU=n$;},bq:function(){if(!aAA){DT.body.removeChild(DD);}}};})();g_pd=(function(){var\n",
      "azy=window.location,nw={},bH,azw=azy.search.substring(1),aAs,aAu;if(!azw)\n",
      "return nw;aAs=azw.split(\"&\");for(bH=0;bH<aAs.length;bH++){aAu=aAs[bH].split('=');nw[aAu[0]]=aAu[1]?aAu[1]:\"\";}\n",
      "return nw;})();g_pc=(function(){var $is_ABP_whitelisted=null;var $Image1=new Image;var $Image2=new Image;var $error1=false;var $error2=false;var $remaining=2;var $random=Math.random()*11;function $imageLoaded(){$remaining--;if($remaining===0)\n",
      "$is_ABP_whitelisted=!$error1&&$error2;}\n",
      "$Image1.onload=$Image2.onload=$imageLoaded;$Image1.onerror=function(){$error1=true;$imageLoaded();};$Image2.onerror=function(){$error2=true;$imageLoaded();};$Image1.src='/px.gif?ch=1&rn='+$random;$Image2.src='/px.gif?ch=2&rn='+$random;return{azo:function(){return'&abp='+($is_ABP_whitelisted?'1':'0');},$isWhitelisted:function(){return $is_ABP_whitelisted;},$onReady:function($callback){function $poll(){if($is_ABP_whitelisted===null)\n",
      "setTimeout($poll,100);else $callback();}\n",
      "$poll();}}})();(function(){var aAm=screen,Rr=window,azy=Rr.location,aAz=top.location,DT=document,Sf=DT.body||DT.getElementsByTagName('body')[0],aAx=0,aAv=0,aAw=0,$IE=null;if(Sf.className==='ie6')\n",
      "$IE=6;else if(Sf.className==='ie7')\n",
      "$IE=7;else if(Sf.className==='ie8')\n",
      "$IE=8;else if(Sf.className==='ie9')\n",
      "$IE=9;function aAt($callback){aAw++;aAx=Rr.innerWidth||DT.documentElement.clientWidth||Sf.clientWidth;aAv=Rr.innerHeight||DT.documentElement.clientHeight||Sf.clientHeight;if(aAx>0||aAw>=5){$callback();}\n",
      "else{setTimeout(aAt,100);}}\n",
      "var $num_requirements=2;function $requirementMet(){$num_requirements--;if($num_requirements===0)\n",
      "aAy();}\n",
      "aAt($requirementMet);g_pc.$onReady($requirementMet);function aAy(){var ef=undefined,IQ=encodeURIComponent,aAr;if(aAz!=azy&&g_pd.r_s===ef)\n",
      "aAz.href=azy.href;aAr=DT.createElement('script');aAr.type='text/javascript';aAr.src='/glp'+'?r='+(g_pd.r!==ef?g_pd.r:(DT.referrer?IQ(DT.referrer.substr(0,255)):''))+\n",
      "(g_pd.r_u?'&u='+g_pd.r_u:'&u='+IQ(azy.href.split('?')[0]))+\n",
      "(g_pd.gc?'&gc='+g_pd.gc:'')+\n",
      "(g_pd.cid?'&cid='+g_pd.cid:'')+\n",
      "(g_pd.query?'&sq='+g_pd.query:'')+\n",
      "(g_pd.search?'&ss=1':'')+\n",
      "(g_pd.a!==ef?'&a':'')+\n",
      "(g_pd.z!==ef?'&z':'')+\n",
      "(g_pd.z_ds!==ef?'&z_ds':'')+\n",
      "(g_pd.r_s!==ef?'&r_s='+g_pd.r_s:'')+\n",
      "(g_pd.r_d!==ef?'&r_d='+g_pd.r_d:'')+'&rw='+aAm.width+'&rh='+aAm.height+\n",
      "(g_pd.r_ww!==ef?'&ww='+g_pd.r_ww:'&ww='+aAx)+\n",
      "(g_pd.r_wh!==ef?'&wh='+g_pd.r_wh:'&wh='+aAv)+\n",
      "(g_pc.$isWhitelisted()?'&abp=1':'')+\n",
      "($IE!==null?'&ie='+$IE:'')+\n",
      "(g_pd.partner!==ef?'&partner='+g_pd.partner:'')+\n",
      "(g_pd.subid1!==ef?'&subid1='+g_pd.subid1:'')+\n",
      "(g_pd.subid2!==ef?'&subid2='+g_pd.subid2:'')+\n",
      "(g_pd.subid3!==ef?'&subid3='+g_pd.subid3:'')+\n",
      "(g_pd.subid4!==ef?'&subid4='+g_pd.subid4:'')+\n",
      "(g_pd.subid5!==ef?'&subid5='+g_pd.subid5:'');Sf.appendChild(aAr);}})();</script></body></html>Symbol|Security Name|Market Category|Test Issue|Financial Status|Round Lot Size|ETF|NextShares\n",
      "AABA|Altaba Inc. - Common Stock|Q|N|N|100|N|N\n"
     ]
    }
   ],
   "source": [
    "!head -43 nasdaq.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSAN|Zosano Pharma Corporation - Common Stock|S|N|N|100|N|N\n",
      "ZUMZ|Zumiez Inc. - Common Stock|Q|N|N|100|N|N\n",
      "ZVZZC|NASDAQ TEST STOCK Nextshares Test Security|G|Y|N|100||Y\n",
      "ZVZZT|NASDAQ TEST STOCK|G|Y|N|100||N\n",
      "ZWZZT|NASDAQ TEST STOCK|S|Y|N|100||N\n",
      "ZXYZ.A|Nasdaq Symbology Test Common Stock|Q|Y|N|100||N\n",
      "ZXZZT|NASDAQ TEST STOCK|G|Y|N|100||N\n",
      "ZYNE|Zynerba Pharmaceuticals, Inc. - Common Stock|G|N|N|100|N|N\n",
      "ZYXI|Zynex, Inc. - Common Stock|S|N|N|100|N|N\n",
      "File Creation Time: 0408201903:03|||||||\n"
     ]
    }
   ],
   "source": [
    "!tail -10 nasdaq.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n +43 nasdaq.lst | cat | sed '$d' | sed 's/|/ /g' > nasdaq.lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AABA Altaba Inc. - Common Stock Q N N 100 N N\n",
      "AAL American Airlines Group, Inc. - Common Stock Q N N 100 N N\n",
      "AAME Atlantic American Corporation - Common Stock G N N 100 N N\n",
      "AAOI Applied Optoelectronics, Inc. - Common Stock G N N 100 N N\n",
      "AAON AAON, Inc. - Common Stock Q N N 100 N N\n",
      "AAPL Apple Inc. - Common Stock Q N N 100 N N\n",
      "AAWW Atlas Air Worldwide Holdings - Common Stock Q N N 100 N N\n",
      "AAXJ iShares MSCI All Country Asia ex Japan Index Fund G N N 100 Y N\n",
      "AAXN Axon Enterprise, Inc. - Common Stock Q N N 100 N N\n",
      "ABCB Ameris Bancorp - Common Stock Q N N 100 N N\n",
      "...\n",
      "ZS Zscaler, Inc. - Common Stock Q N N 100 N N\n",
      "ZSAN Zosano Pharma Corporation - Common Stock S N N 100 N N\n",
      "ZUMZ Zumiez Inc. - Common Stock Q N N 100 N N\n",
      "ZVZZC NASDAQ TEST STOCK Nextshares Test Security G Y N 100  Y\n",
      "ZVZZT NASDAQ TEST STOCK G Y N 100  N\n",
      "ZWZZT NASDAQ TEST STOCK S Y N 100  N\n",
      "ZXYZ.A Nasdaq Symbology Test Common Stock Q Y N 100  N\n",
      "ZXZZT NASDAQ TEST STOCK G Y N 100  N\n",
      "ZYNE Zynerba Pharmaceuticals, Inc. - Common Stock G N N 100 N N\n",
      "ZYXI Zynex, Inc. - Common Stock S N N 100 N N\n"
     ]
    }
   ],
   "source": [
    "!echo; head nasdaq.lst2; echo \"...\"; tail nasdaq.lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print $1}' nasdaq.lst2 > nasdaq.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = pd.read_csv('nasdaq.csv', index_col=None, header=None)\n",
    "tickers.columns = ['ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get NYSE Symbols\n",
    "nyse = pd.read_csv('companylist.csv')\n",
    "cols = [1,2,3,4,5,6,7,8,9]\n",
    "nyse.drop(nyse.columns[cols],axis=1,inplace=True)\n",
    "nyse.columns = ['ticker']\n",
    "tickers = tickers.append(nyse)\n",
    "tickers = list(tickers['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets associated with more than one stock: 542,968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2601709, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Tweets associated with more than one stock...\n",
    "\n",
    "# First, using the tweet id...\n",
    "g = df.groupby('tweet_id').size().reset_index(name='count')\n",
    "print('Tweets associated with more than one stock:',\"{:,}\".format(sum(g[g['count']>1]['count'])))\n",
    "\n",
    "# Delete those tweet ids\n",
    "tweets_ids_to_drop = list(g[g['count']>1]['tweet_id'])\n",
    "df = df[~df.tweet_id.isin(tweets_ids_to_drop)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "# This one takes a while when run for the first time\n",
    "\n",
    "import glove_helper; reload(glove_helper)\n",
    "\n",
    "hands = glove_helper.Hands(ndim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Second, discard tweets that mention multiple stocks\n",
    "# This one takes a while, hang in there\n",
    "# Take advantage of the whole iteration and transform tweet for classifier\n",
    "df_duplicate = df.copy()\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "indices_to_drop = []\n",
    "\n",
    "for index, row in df_duplicate.iterrows():\n",
    "    \n",
    "    print(index,end='\\r')\n",
    "\n",
    "    tweet_no_url = re.sub(r'http\\S+', '', row['tweet'])\n",
    "    tokens = tweet_no_url.split(' ')\n",
    "    sentence = '<s> '\n",
    "    flag = True\n",
    "    prev_token = ''\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            a = hands.vocab.word_to_id[token.lower()]\n",
    "            sentence = sentence + token.lower() + ' '\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if token in tickers and prev_token == '$':\n",
    "\n",
    "            if token != row['ticker']:\n",
    "                indices_to_drop.append(index)\n",
    "                flag=False\n",
    "                break\n",
    "        \n",
    "        prev_token = token\n",
    "    \n",
    "    sentence += '</s>'\n",
    "    \n",
    "    if flag:\n",
    "        \n",
    "        df_duplicate.at[index,'tweet'] = re.sub('[^A-Za-z0-9 <>/]+', '', sentence)\n",
    "\n",
    "df_duplicate.drop(indices_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicate.to_csv('../w266_final/clean_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1105519541147262976</td>\n",
       "      <td>C</td>\n",
       "      <td>2019-03-12 17:22</td>\n",
       "      <td>&lt;s&gt;  c  the only big bank i buy  &lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1105519387367227393</td>\n",
       "      <td>C</td>\n",
       "      <td>2019-03-12 17:22</td>\n",
       "      <td>&lt;s&gt; on the density of  &lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1105517203074371584</td>\n",
       "      <td>C</td>\n",
       "      <td>2019-03-12 17:13</td>\n",
       "      <td>&lt;s&gt; ucla is the of  c is so good at spinning a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1105288514155929601</td>\n",
       "      <td>C</td>\n",
       "      <td>2019-03-12 02:04</td>\n",
       "      <td>&lt;s&gt; &lt; &lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1104039312251858945</td>\n",
       "      <td>DWDP</td>\n",
       "      <td>2019-03-08 15:20</td>\n",
       "      <td>&lt;s&gt; in inc  after decline in shorted  &lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id ticker         timestamp  \\\n",
       "200  1105519541147262976      C  2019-03-12 17:22   \n",
       "201  1105519387367227393      C  2019-03-12 17:22   \n",
       "203  1105517203074371584      C  2019-03-12 17:13   \n",
       "290  1105288514155929601      C  2019-03-12 02:04   \n",
       "323  1104039312251858945   DWDP  2019-03-08 15:20   \n",
       "\n",
       "                                                 tweet  \n",
       "200              <s>  c  the only big bank i buy  </s>  \n",
       "201                        <s> on the density of  </s>  \n",
       "203  <s> ucla is the of  c is so good at spinning a...  \n",
       "290                                         <s> < </s>  \n",
       "323         <s> in inc  after decline in shorted  </s>  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_duplicate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
